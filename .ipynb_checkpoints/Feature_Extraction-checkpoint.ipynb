{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class feature_extract:\n",
    "    def content(self,url):\n",
    "        a=url.split(\"/\")\n",
    "        return a[3]\n",
    "    def title_word_count(self,title):\n",
    "        return len(title.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "data=pd.read_excel('july.xlsx')\n",
    "p=data['Content'].values\n",
    "\n",
    "urls=data['URL'].values\n",
    "print(len(urls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "yes\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Getting article from URL\n",
    "\n",
    "import os.path\n",
    "\n",
    "save_path=r'C:\\Users\\Manoj\\Desktop\\Final_Year_Project\\JULY'\n",
    "i=2\n",
    "for url in urls:\n",
    "    if int(p[i-2])!=1:\n",
    "        print(\"Hii\")\n",
    "        print(i)\n",
    "        spl=url.split(\"/\")\n",
    "        completeName = os.path.join(save_path,str(i)+\".txt\")\n",
    "        r = requests.get(url) \n",
    "        soup = bs(r.content, 'html5lib')\n",
    "        if spl[2]==\"www.ndtv.com\":\n",
    "            a=soup.find('div',class_='ins_storybody')\n",
    "            if a is not None:\n",
    "                \n",
    "                f=open(completeName,'w+',encoding='utf-8')\n",
    "                f.write(a.get_text())\n",
    "                f.close()\n",
    "            i+=1\n",
    "        elif spl[2]==\"sports.ndtv.com\":\n",
    "            print(\"yes\")\n",
    "            a=soup.find('div',itemprop='articleBody')\n",
    "            if a is not None:\n",
    "                f=open(completeName,'w+',encoding='utf-8')\n",
    "                f.write(a.get_text())\n",
    "                f.close()\n",
    "            i+=1\n",
    "    p[i-2]=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n"
     ]
    }
   ],
   "source": [
    "#Getting title from URL\n",
    "import pandas as pd\n",
    "title=pd.DataFrame(columns=['seqno','title','subtitle','images','videos','hyperlinks'])\n",
    "i=2\n",
    "li=[]\n",
    "for url in urls[:len(urls)-3]:\n",
    "    print(i)\n",
    "    spl=url.split(\"/\")\n",
    "    r = requests.get(url) \n",
    "    soup = bs(r.content, 'html5lib')\n",
    "    tit=soup.find('title')\n",
    "    tit=tit.get_text()\n",
    "    img=list(soup.find_all('img'))\n",
    "    img=len(img)\n",
    "    video=list(soup.find_all('video'))\n",
    "    anc=list(soup.find_all('a'))\n",
    "    anc=len(anc)\n",
    "    if spl[2]==\"www.ndtv.com\":\n",
    "        sub=soup.find('h2',class_=\"ins_descp\")\n",
    "        \n",
    "    elif spl[2]==\"sports.ndtv.com\":\n",
    "        sub=soup.find('div',class_=\"post-description\")\n",
    "    p=[i,tit,sub,img,video,anc]\n",
    "    li.append(p)\n",
    "    i+=1\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqno</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>images</th>\n",
       "      <th>videos</th>\n",
       "      <th>hyperlinks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Election 2019: After Asansol Violence, Babul S...</td>\n",
       "      <td>&lt;h2 class=\"ins_descp\"&gt; After reports of Babul ...</td>\n",
       "      <td>17</td>\n",
       "      <td>[]</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Trivandrum International Airport, Kerala: Gold...</td>\n",
       "      <td>&lt;h2 class=\"ins_descp\"&gt; Trivandrum Internationa...</td>\n",
       "      <td>16</td>\n",
       "      <td>[]</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Lok Sabha Elections 2019: Jaya Bachchan, Those...</td>\n",
       "      <td>&lt;h2 class=\"ins_descp\"&gt; Jaya Bachchan, who was ...</td>\n",
       "      <td>16</td>\n",
       "      <td>[]</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Cyclone Fani Highlights: Cyclone Strengthens, ...</td>\n",
       "      <td>&lt;h2 class=\"ins_descp\"&gt; Cyclone Fani: The Met d...</td>\n",
       "      <td>13</td>\n",
       "      <td>[]</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Woman Called Out In Gurgaon Mall Over \"Short D...</td>\n",
       "      <td>&lt;h2 class=\"ins_descp\"&gt; \"This middle aged woman...</td>\n",
       "      <td>16</td>\n",
       "      <td>[]</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>Hockey India Recommends PR Sreejesh For Rajiv ...</td>\n",
       "      <td>&lt;div class=\"post-description\" itemprop=\"descri...</td>\n",
       "      <td>16</td>\n",
       "      <td>[]</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>Lok Sabha Elections 2019: Priyanka Gandhi Vadr...</td>\n",
       "      <td>&lt;h2 class=\"ins_descp\"&gt; Priyanka Gandhi Vadra w...</td>\n",
       "      <td>15</td>\n",
       "      <td>[]</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>Shreyas Gopal Pleased To Dismiss Virat Kohli A...</td>\n",
       "      <td>&lt;div class=\"post-description\" itemprop=\"descri...</td>\n",
       "      <td>84</td>\n",
       "      <td>[]</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>Gautam Gambhir Was \"Insecure, Negative And Pes...</td>\n",
       "      <td>&lt;div class=\"post-description\" itemprop=\"descri...</td>\n",
       "      <td>79</td>\n",
       "      <td>[]</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>PM Narendra Modi Condemns Maoist Attack In Mah...</td>\n",
       "      <td>&lt;h2 class=\"ins_descp\"&gt; The attack took place h...</td>\n",
       "      <td>15</td>\n",
       "      <td>[]</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seqno                                              title  \\\n",
       "0      2  Election 2019: After Asansol Violence, Babul S...   \n",
       "1      3  Trivandrum International Airport, Kerala: Gold...   \n",
       "2      4  Lok Sabha Elections 2019: Jaya Bachchan, Those...   \n",
       "3      5  Cyclone Fani Highlights: Cyclone Strengthens, ...   \n",
       "4      6  Woman Called Out In Gurgaon Mall Over \"Short D...   \n",
       "5      7  Hockey India Recommends PR Sreejesh For Rajiv ...   \n",
       "6      8  Lok Sabha Elections 2019: Priyanka Gandhi Vadr...   \n",
       "7      9  Shreyas Gopal Pleased To Dismiss Virat Kohli A...   \n",
       "8     10  Gautam Gambhir Was \"Insecure, Negative And Pes...   \n",
       "9     11  PM Narendra Modi Condemns Maoist Attack In Mah...   \n",
       "\n",
       "                                            subtitle  images videos  \\\n",
       "0  <h2 class=\"ins_descp\"> After reports of Babul ...      17     []   \n",
       "1  <h2 class=\"ins_descp\"> Trivandrum Internationa...      16     []   \n",
       "2  <h2 class=\"ins_descp\"> Jaya Bachchan, who was ...      16     []   \n",
       "3  <h2 class=\"ins_descp\"> Cyclone Fani: The Met d...      13     []   \n",
       "4  <h2 class=\"ins_descp\"> \"This middle aged woman...      16     []   \n",
       "5  <div class=\"post-description\" itemprop=\"descri...      16     []   \n",
       "6  <h2 class=\"ins_descp\"> Priyanka Gandhi Vadra w...      15     []   \n",
       "7  <div class=\"post-description\" itemprop=\"descri...      84     []   \n",
       "8  <div class=\"post-description\" itemprop=\"descri...      79     []   \n",
       "9  <h2 class=\"ins_descp\"> The attack took place h...      15     []   \n",
       "\n",
       "   hyperlinks  \n",
       "0         125  \n",
       "1         120  \n",
       "2         121  \n",
       "3         223  \n",
       "4         122  \n",
       "5         163  \n",
       "6         122  \n",
       "7         311  \n",
       "8         301  \n",
       "9         124  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title=pd.DataFrame(li,columns=['seqno','title','subtitle','images','videos','hyperlinks'])\n",
    "title.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n",
      "(182, 6)\n",
      "(182, 1)\n"
     ]
    }
   ],
   "source": [
    "weekday={0:\"Monday\",1:\"Tuesday\",2:\"Wednesday\",3:\"Thursday\",4:\"Friday\",5:\"Saturday\",6:\"Sunday\"}\n",
    "import datetime\n",
    "lis=data['date'].values\n",
    "print(len(lis))\n",
    "print(title.shape)\n",
    "dates=[]\n",
    "for da in lis[:182]:\n",
    "    a=datetime.datetime(2019,5,int(da))\n",
    "    dates.append(weekday[a.weekday()])\n",
    "\n",
    "dat=pd.DataFrame(dates,columns=['day'])\n",
    "print(dat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.concat([dat,title],axis=1)\n",
    "final.head(10)\n",
    "final.to_csv('feature_set1(MAY).csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "completeName = os.path.join(save_path,\"1.txt\")\n",
    "r = requests.get(urls[0]) \n",
    "\n",
    "soup = bs(r.content, 'html5lib')\n",
    "\n",
    "obj=feature_extract()\n",
    "\n",
    "newstype=obj.content(urls[0])\n",
    "\n",
    "titlewordcount=obj.title_word_count(str(soup.find('title')))\n",
    "i=0\n",
    "a=soup.find('div',class_='ins_storybody')\n",
    "if a is not None:\n",
    "    print(i+1)\n",
    "    f=open(completeName,'w+')\n",
    "    f.write(a.get_text())\n",
    "    f.close()\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
